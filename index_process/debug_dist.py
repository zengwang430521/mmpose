# import torch
# from function import LocalAttention
# # kH and kW for local patch size
# # works only on GPU
# module = LocalAttention(inp_channels=3, out_channels=16, kH=7, kW=7).cuda()
# x = torch.rand(32, 3, 64, 64).cuda()
#
# # Q, K, V are generated by convolutions of x
# y = module(x)
# print(y)



import torch
from function import f_distance
import time

def indexed_dist(x1, x2, idx):
    B, N, K = idx.shape
    C = x1.shape[-1]
    idx_batch = torch.arange(B)[:, None, None].expand([B, N, K])
    xk = x2[idx_batch.reshape(-1), idx.reshape(-1), :]
    xk = xk.reshape(B, N, K, C)
    dist = x1[:, :, None, :] - xk
    dist = (dist ** 2).mean(-1).sqrt()
    return dist


B, N, M, C, K = 16, 256, 64, 64, 16
x1 = torch.rand([B, N, C]).cuda()
x2 = torch.rand([B, M, C]).cuda()
idx = torch.rand([B, N, K]) * (M-1)
idx = idx.round().long().cuda().clamp(0, M-1)

t0 = time.time()
dist = f_distance(x1, x2, idx.int())
t1 = time.time()
dist_gt = indexed_dist(x1, x2, idx)
t2 = time.time()

err = dist - dist_gt
# print(dist)
print(err.abs().max())
print(t1-t0)
print(t2-t1)
t=0