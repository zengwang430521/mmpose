# import torch
# from function import LocalAttention
# # kH and kW for local patch size
# # works only on GPU
# module = LocalAttention(inp_channels=3, out_channels=16, kH=7, kW=7).cuda()
# x = torch.rand(32, 3, 64, 64).cuda()
#
# # Q, K, V are generated by convolutions of x
# y = module(x)
# print(y)



import torch
from function import f_distance, f_attn, f_weighted_sum
import time
# from function import attn_backward_query, attn_backward_key


def indexed_dist(x1, x2, idx):
    B, N, K = idx.shape
    C = x1.shape[-1]
    idx_batch = torch.arange(B)[:, None, None].expand([B, N, K])
    xk = x2[idx_batch.reshape(-1), idx.reshape(-1), :]
    xk = xk.reshape(B, N, K, C)
    dist = x1[:, :, None, :] - xk
    dist = (dist ** 2).sum(-1).sqrt()
    return dist


def indexed_attn(query, key, idx):
    B, N, K = idx.shape
    C = query.shape[-1]
    idx_batch = torch.arange(B)[:, None, None].expand([B, N, K])
    xk = key[idx_batch.reshape(-1), idx.reshape(-1), :]
    xk = xk.reshape(B, N, K, C)
    attn = query[:, :, None, :] * xk
    attn = attn.sum(dim=-1)
    return attn


def indexed_back_query(attn, key, idx):
    B, N, K = idx.shape
    C = key.shape[-1]
    out = attn.new_zeros(B, N, C)

    idx_batch = torch.arange(B)[:, None, None].expand([B, N, K]).to(key.device)
    xk = key[idx_batch.reshape(-1), idx.reshape(-1), :]
    xk = xk.reshape(B, N, K, C)

    out = xk * attn.unsqueeze(-1)   # B, N, K, C
    out = out.sum(dim=2)
    return out


def indexed_back_key(attn, query, idx, Nk):
    B, N, K = idx.shape
    C = query.shape[-1]
    out = attn.new_zeros(B, Nk, C)

    idx_batch = torch.arange(B).to(query.device)
    grad_k = query[:, :, None, :] * attn[:, :, :, None]     # B, N, K, C
    for n in range(N):
        for k in range(K):
            idx_key = idx[:, n, k]
            out[idx_batch, idx_key, :] = out[idx_batch, idx_key, :] + grad_k[:, n, k, :]
    return out


# B, N, M, C, K = 16, 256, 64, 64, 16
# x1 = torch.rand([B, N, C]).cuda()
# x2 = torch.rand([B, M, C]).cuda()
# idx = torch.rand([B, N, K]) * (M-1)
# idx = idx.round().long().cuda().clamp(0, M-1)
#
# t0 = time.time()
# dist = f_distance(x1, x2, idx.int())
# t1 = time.time()
# dist_gt = indexed_dist(x1, x2, idx)
# t2 = time.time()
#
# err = dist - dist_gt
# # print(dist)
# print(err.abs().max())
# print(t1-t0)
# print(t2-t1)
# print((t2-t1) / (t1-t0))
#
#
#
# t0 = time.time()
# attn = f_attn(x1, x2, idx.int())
# t1 = time.time()
# attn_gt = indexed_attn(x1, x2, idx)
# t2 = time.time()
#
# err = attn - attn_gt
# # print(dist)
# print(err.abs().max())
# print(t1-t0)
# print(t2-t1)
# print((t2-t1) / (t1-t0))










# # test backwrads
# # B, N, M, C, K = 1, 4, 3, 3, 2
# B, N, M, C, K = 8, 256, 128, 64, 49
#
# q = torch.rand([B, N, C]).cuda() - 0.5
# k = torch.rand([B, M, C]).cuda() - 0.5
# idx = torch.rand([B, N, K]) * (M-1)
# idx = idx.round().long().cuda().clamp(0, M-1)
#
# q1 = q.clone().requires_grad_(True)
# k1 = k.clone().requires_grad_(True)
#
# t0 = time.time()
# attn1 = f_attn(q1, k1, idx.int())
# attn1.retain_grad()
# l1 = attn1.sum()
# # t = attn1.grad_fn.saved_tensors
# # grad_t = attn1.new_ones(attn1.shape)
# # q_grad1 = attn_backward_query(grad_t, t[1], t[2])
# l1.backward()
# t1 = time.time()
#
#
#
# q2 = q.clone().requires_grad_(True)
# k2 = k.clone().requires_grad_(True)
# t2 = time.time()
# attn2 = indexed_attn(q2, k2, idx.long())
# attn2.retain_grad()
# l2 = attn2.sum()
# l2.backward()
# t3 = time.time()
#
#
# print(t1-t0)
# print(t3-t2)
# print((t3-t2) / (t1-t0))
#
#
# err = attn1 - attn2
# print('attn:')
# print(err.abs().max())
#
# err = q1.grad - q2.grad
# print('err_q:')
# print(err.abs().max())
#
# err = k1.grad - k2.grad
# print('err_k:')
# print(err.abs().max())



# k_grad2 = indexed_back_key(attn2.grad, q2, idx, M)
# err = k_grad2 - k2.grad
# print('err_k2:')
# print(err.abs().max())
#
# # k_grad1 = attn_backward_key(attn1.grad.contiguous(), q1.contiguous(), idx.int().contiguous(), k1.shape[1])
# k_grad1 = attn_backward_key(attn1.grad.contiguous(), q1, idx.int(), k1.shape[1])
# err = k_grad1 - k2.grad
# print('err_k1:')
# print(err.abs().max())



# q_grad1 = attn_backward_query(attn1.grad, k1, idx.int())
# print('q_grad1 is:')
# print(q_grad1)
#
# err = q1.grad - q_grad1
# print('err is:')
# print(err)

# q_grad1 = attn_backward_query(attn1.grad, k1, idx.int())
# q_grad2 = indexed_back_query(attn2.grad, k2, idx)
#
# err = q_grad1 - q2.grad
# print('err:')
# print(err.abs().max())
#
# err = q_grad2 - q2.grad
# print('err:')
# print(err.abs().max())
#
#
# print('finish')







# test backwrads
# B, N, M, C, K = 1, 4, 3, 3, 2



B, N, M, C, K = 8, 256, 128, 64, 49

q = torch.rand([B, N, C]).cuda() - 0.5
k = torch.rand([B, M, C]).cuda() - 0.5
v = torch.rand([B, M, C]).cuda() - 0.5

idx = torch.rand([B, N, K]) * (M-1)
idx = idx.round().long().cuda().clamp(0, M-1)

q1 = q.clone().requires_grad_(True)
k1 = k.clone().requires_grad_(True)
v1 = v.clone().requires_grad_(True)

t0 = time.time()
attn1 = f_attn(q1, k1, idx.int())
# attn1.retain_grad()
attn1 = attn1.softmax(dim=-1)
out1 = f_weighted_sum(attn1, v1, idx)
l1 = out1.sum()
l1.backward()
t1 = time.time()


q2 = q.clone().requires_grad_(True)
k2 = k.clone().requires_grad_(True)
v2 = v.clone().requires_grad_(True)

t2 = time.time()
attn2 = indexed_attn(q2, k2, idx.long())
attn2 = attn2.softmax(dim=-1)
# attn2.retain_grad()
out2 = indexed_back_query(attn2, v2, idx)
l2 = out2.sum()
l2.backward()
t3 = time.time()

print('t1:')
print(t1-t0)
print('t2:')
print(t3-t2)
print('t2/t1:')
print((t3-t2) / (t1-t0))


err = attn1 - attn2
print('attn err:')
print(err.abs().max())

err = out1 - out2
print('out err:')
print(err.abs().max())

err = q1.grad - q2.grad
print('err_q:')
print(err.abs().max())

err = k1.grad - k2.grad
print('err_k:')
print(err.abs().max())
